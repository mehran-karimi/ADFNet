{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe1d50d-a160-49c2-88a3-ec740e9d1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# -------------------------\n",
    "# User-configurable params\n",
    "# -------------------------\n",
    "INPUT_DIR = \"./input\"\n",
    "MODEL_DIR = \"./model\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "LOG_DIR = \"./logs\"\n",
    "\n",
    "# Control how many CSVs to load for quick testing; set to None to load all                 =3\n",
    "MAX_FILES_TO_LOAD = None\n",
    "\n",
    "# Task mode: \"auto\" (detect), \"binary\", or \"multiclass\"\n",
    "TASK_MODE = \"multiclass\"\n",
    "\n",
    "# Training hyperparams\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "ATTENTION_D = 128\n",
    "EARLY_EXIT_THRESHOLDS = (0.95, 0.98)  # (exit1_thresh, exit2_thresh)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Make dirs\n",
    "Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(LOG_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca01e75-7e5b-483a-9841-5d6b7f7a71d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 95 CSV files...\n",
      " - input\\ciciot2023_5percent_sample_part_1.csv\n",
      " - input\\ciciot2023_5percent_sample_part_10.csv\n",
      " - input\\ciciot2023_5percent_sample_part_11.csv\n",
      " - input\\ciciot2023_5percent_sample_part_12.csv\n",
      " - input\\ciciot2023_5percent_sample_part_13.csv\n",
      " - input\\ciciot2023_5percent_sample_part_14.csv\n",
      " - input\\ciciot2023_5percent_sample_part_15.csv\n",
      " - input\\ciciot2023_5percent_sample_part_16.csv\n",
      " - input\\ciciot2023_5percent_sample_part_17.csv\n",
      " - input\\ciciot2023_5percent_sample_part_18.csv\n",
      " - input\\ciciot2023_5percent_sample_part_19.csv\n",
      " - input\\ciciot2023_5percent_sample_part_2.csv\n",
      " - input\\ciciot2023_5percent_sample_part_20.csv\n",
      " - input\\ciciot2023_5percent_sample_part_21.csv\n",
      " - input\\ciciot2023_5percent_sample_part_22.csv\n",
      " - input\\ciciot2023_5percent_sample_part_23.csv\n",
      " - input\\ciciot2023_5percent_sample_part_24.csv\n",
      " - input\\ciciot2023_5percent_sample_part_25.csv\n",
      " - input\\ciciot2023_5percent_sample_part_26.csv\n",
      " - input\\ciciot2023_5percent_sample_part_27.csv\n",
      " - input\\ciciot2023_5percent_sample_part_28.csv\n",
      " - input\\ciciot2023_5percent_sample_part_29.csv\n",
      " - input\\ciciot2023_5percent_sample_part_3.csv\n",
      " - input\\ciciot2023_5percent_sample_part_30.csv\n",
      " - input\\ciciot2023_5percent_sample_part_31.csv\n",
      " - input\\ciciot2023_5percent_sample_part_32.csv\n",
      " - input\\ciciot2023_5percent_sample_part_33.csv\n",
      " - input\\ciciot2023_5percent_sample_part_34.csv\n",
      " - input\\ciciot2023_5percent_sample_part_35.csv\n",
      " - input\\ciciot2023_5percent_sample_part_36.csv\n",
      " - input\\ciciot2023_5percent_sample_part_37.csv\n",
      " - input\\ciciot2023_5percent_sample_part_38.csv\n",
      " - input\\ciciot2023_5percent_sample_part_39.csv\n",
      " - input\\ciciot2023_5percent_sample_part_4.csv\n",
      " - input\\ciciot2023_5percent_sample_part_40.csv\n",
      " - input\\ciciot2023_5percent_sample_part_41.csv\n",
      " - input\\ciciot2023_5percent_sample_part_42.csv\n",
      " - input\\ciciot2023_5percent_sample_part_43.csv\n",
      " - input\\ciciot2023_5percent_sample_part_44.csv\n",
      " - input\\ciciot2023_5percent_sample_part_45.csv\n",
      " - input\\ciciot2023_5percent_sample_part_46.csv\n",
      " - input\\ciciot2023_5percent_sample_part_47.csv\n",
      " - input\\ciciot2023_5percent_sample_part_48.csv\n",
      " - input\\ciciot2023_5percent_sample_part_49.csv\n",
      " - input\\ciciot2023_5percent_sample_part_5.csv\n",
      " - input\\ciciot2023_5percent_sample_part_50.csv\n",
      " - input\\ciciot2023_5percent_sample_part_51.csv\n",
      " - input\\ciciot2023_5percent_sample_part_52.csv\n",
      " - input\\ciciot2023_5percent_sample_part_53.csv\n",
      " - input\\ciciot2023_5percent_sample_part_54.csv\n",
      " - input\\ciciot2023_5percent_sample_part_55.csv\n",
      " - input\\ciciot2023_5percent_sample_part_56.csv\n",
      " - input\\ciciot2023_5percent_sample_part_57.csv\n",
      " - input\\ciciot2023_5percent_sample_part_58.csv\n",
      " - input\\ciciot2023_5percent_sample_part_59.csv\n",
      " - input\\ciciot2023_5percent_sample_part_6.csv\n",
      " - input\\ciciot2023_5percent_sample_part_60.csv\n",
      " - input\\ciciot2023_5percent_sample_part_61.csv\n",
      " - input\\ciciot2023_5percent_sample_part_62.csv\n",
      " - input\\ciciot2023_5percent_sample_part_63.csv\n",
      " - input\\ciciot2023_5percent_sample_part_64.csv\n",
      " - input\\ciciot2023_5percent_sample_part_65.csv\n",
      " - input\\ciciot2023_5percent_sample_part_66.csv\n",
      " - input\\ciciot2023_5percent_sample_part_67.csv\n",
      " - input\\ciciot2023_5percent_sample_part_68.csv\n",
      " - input\\ciciot2023_5percent_sample_part_69.csv\n",
      " - input\\ciciot2023_5percent_sample_part_7.csv\n",
      " - input\\ciciot2023_5percent_sample_part_70.csv\n",
      " - input\\ciciot2023_5percent_sample_part_71.csv\n",
      " - input\\ciciot2023_5percent_sample_part_72.csv\n",
      " - input\\ciciot2023_5percent_sample_part_73.csv\n",
      " - input\\ciciot2023_5percent_sample_part_74.csv\n",
      " - input\\ciciot2023_5percent_sample_part_75.csv\n",
      " - input\\ciciot2023_5percent_sample_part_76.csv\n",
      " - input\\ciciot2023_5percent_sample_part_77.csv\n",
      " - input\\ciciot2023_5percent_sample_part_78.csv\n",
      " - input\\ciciot2023_5percent_sample_part_79.csv\n",
      " - input\\ciciot2023_5percent_sample_part_8.csv\n",
      " - input\\ciciot2023_5percent_sample_part_80.csv\n",
      " - input\\ciciot2023_5percent_sample_part_81.csv\n",
      " - input\\ciciot2023_5percent_sample_part_82.csv\n",
      " - input\\ciciot2023_5percent_sample_part_83.csv\n",
      " - input\\ciciot2023_5percent_sample_part_84.csv\n",
      " - input\\ciciot2023_5percent_sample_part_85.csv\n",
      " - input\\ciciot2023_5percent_sample_part_86.csv\n",
      " - input\\ciciot2023_5percent_sample_part_87.csv\n",
      " - input\\ciciot2023_5percent_sample_part_88.csv\n",
      " - input\\ciciot2023_5percent_sample_part_89.csv\n",
      " - input\\ciciot2023_5percent_sample_part_9.csv\n",
      " - input\\ciciot2023_5percent_sample_part_90.csv\n",
      " - input\\ciciot2023_5percent_sample_part_91.csv\n",
      " - input\\ciciot2023_5percent_sample_part_92.csv\n",
      " - input\\ciciot2023_5percent_sample_part_93.csv\n",
      " - input\\ciciot2023_5percent_sample_part_94.csv\n",
      " - input\\sampling_info.csv\n",
      "Combined shape: (4668746, 52)\n",
      "Detected label column: label\n",
      "Unique labels count: 35\n",
      "Sample unique labels (up to 20): ['DDoS-SynonymousIP_Flood' 'DoS-UDP_Flood' 'DDoS-SYN_Flood'\n",
      " 'DDoS-ICMP_Flood' 'DDoS-RSTFINFlood' 'DDoS-UDP_Flood' 'DDoS-PSHACK_Flood'\n",
      " 'Mirai-udpplain' 'Recon-PortScan' 'DDoS-TCP_Flood' 'Mirai-greip_flood'\n",
      " 'Mirai-greeth_flood' 'DDoS-ICMP_Fragmentation' 'DoS-TCP_Flood'\n",
      " 'DDoS-ACK_Fragmentation' 'DoS-SYN_Flood' 'XSS' 'MITM-ArpSpoofing'\n",
      " 'DNS_Spoofing' 'BenignTraffic']\n",
      "TASK_MODE_USED = multiclass\n",
      "Class mapping (label -> int): {'Backdoor_Malware': 0, 'BenignTraffic': 1, 'BrowserHijacking': 2, 'CommandInjection': 3, 'DDoS-ACK_Fragmentation': 4, 'DDoS-HTTP_Flood': 5, 'DDoS-ICMP_Flood': 6, 'DDoS-ICMP_Fragmentation': 7, 'DDoS-PSHACK_Flood': 8, 'DDoS-RSTFINFlood': 9, 'DDoS-SYN_Flood': 10, 'DDoS-SlowLoris': 11, 'DDoS-SynonymousIP_Flood': 12, 'DDoS-TCP_Flood': 13, 'DDoS-UDP_Flood': 14, 'DDoS-UDP_Fragmentation': 15, 'DNS_Spoofing': 16, 'DictionaryBruteForce': 17, 'DoS-HTTP_Flood': 18, 'DoS-SYN_Flood': 19, 'DoS-TCP_Flood': 20, 'DoS-UDP_Flood': 21, 'MITM-ArpSpoofing': 22, 'Mirai-greeth_flood': 23, 'Mirai-greip_flood': 24, 'Mirai-udpplain': 25, 'Recon-HostDiscovery': 26, 'Recon-OSScan': 27, 'Recon-PingSweep': 28, 'Recon-PortScan': 29, 'SqlInjection': 30, 'Uploading_Attack': 31, 'VulnerabilityScan': 32, 'XSS': 33, 'nan': 34}\n",
      "Numeric feature count: 49\n",
      "Dropping columns with >50% NaN: ['original_records', 'cleaned_records', 'sampled_records']\n",
      "Class distribution: {6: 721306, 14: 540637, 13: 450241, 8: 409374, 10: 405991, 9: 403958, 12: 358669, 21: 332129, 20: 267381, 19: 203095, 1: 110409, 23: 99256, 25: 88670, 24: 75258, 7: 45089, 22: 30722, 15: 28807, 4: 28313, 16: 17836, 26: 13413, 27: 9858, 29: 8032, 18: 7143, 32: 3779, 5: 2902, 11: 2341, 17: 1304, 2: 577, 3: 537, 30: 526, 33: 375, 0: 291, 28: 227, 34: 169, 31: 131}\n",
      "Train/Test shapes: (3734996, 46) (933750, 46)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Utility helpers\n",
    "# -------------------------\n",
    "def to_py(x):\n",
    "    \"\"\"Convert numpy scalars to Python native (so json can serialize).\"\"\"\n",
    "    if isinstance(x, (np.generic, )):\n",
    "        return x.item()\n",
    "    try:\n",
    "        return float(x) if isinstance(x, float) or isinstance(x, np.floating) else int(x)\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# Load CSVs\n",
    "# -------------------------\n",
    "csv_files = sorted(Path(INPUT_DIR).glob(\"**/*.csv\"))\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"No CSV files found under {INPUT_DIR}. Please place CICIDS2023 CSVs there.\")\n",
    "\n",
    "if MAX_FILES_TO_LOAD is not None:\n",
    "    csv_files = csv_files[:MAX_FILES_TO_LOAD]\n",
    "\n",
    "print(f\"Loading {len(csv_files)} CSV files...\")\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    print(\" -\", f)\n",
    "    dfs.append(pd.read_csv(f))\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Combined shape:\", data.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Detect label column\n",
    "# -------------------------\n",
    "label_col = None\n",
    "for c in data.columns:\n",
    "    if c.lower().find(\"label\") >= 0:\n",
    "        label_col = c\n",
    "        break\n",
    "if label_col is None:\n",
    "    label_col = data.columns[-1]\n",
    "print(\"Detected label column:\", label_col)\n",
    "\n",
    "# Inspect unique labels\n",
    "unique_labels = data[label_col].unique()\n",
    "print(\"Unique labels count:\", len(unique_labels))\n",
    "print(\"Sample unique labels (up to 20):\", unique_labels[:20])\n",
    "\n",
    "# -------------------------\n",
    "# Prepare labels according to TASK_MODE\n",
    "# -------------------------\n",
    "if TASK_MODE == \"auto\":\n",
    "    if len(unique_labels) == 2:\n",
    "        TASK_MODE_USED = \"binary\"\n",
    "    else:\n",
    "        TASK_MODE_USED = \"multiclass\"\n",
    "else:\n",
    "    TASK_MODE_USED = TASK_MODE\n",
    "\n",
    "print(\"TASK_MODE_USED =\", TASK_MODE_USED)\n",
    "\n",
    "if TASK_MODE_USED == \"binary\":\n",
    "    # Convert to binary: BenignTraffic -> 0, others -> 1\n",
    "    # Try common benign names; otherwise treat exact 'Benign' or 'BenignTraffic'\n",
    "    def map_to_binary(lbl):\n",
    "        if isinstance(lbl, str):\n",
    "            low = lbl.strip().lower()\n",
    "            if \"benign\" in low or \"normal\" in low:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            # numeric label? treat 0 as benign, others as attack\n",
    "            return 0 if lbl == 0 else 1\n",
    "    data['label_enc'] = data[label_col].apply(map_to_binary).astype(np.int32)\n",
    "    class_mapping = {\"Benign/Normal\":0, \"Attack\":1}\n",
    "else:\n",
    "    le = LabelEncoder()\n",
    "    data['label_enc'] = le.fit_transform(data[label_col].astype(str)).astype(np.int32)\n",
    "    class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(\"Class mapping (label -> int):\", class_mapping)\n",
    "\n",
    "# -------------------------\n",
    "# Feature selection: numeric columns\n",
    "# -------------------------\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove label_enc if present in numeric_cols\n",
    "if 'label_enc' in numeric_cols:\n",
    "    numeric_cols.remove('label_enc')\n",
    "\n",
    "print(\"Numeric feature count:\", len(numeric_cols))\n",
    "\n",
    "# Drop columns with >50% NaN\n",
    "nan_frac = data[numeric_cols].isna().mean()\n",
    "drop_cols = nan_frac[nan_frac > 0.5].index.tolist()\n",
    "if drop_cols:\n",
    "    print(\"Dropping columns with >50% NaN:\", drop_cols)\n",
    "    numeric_cols = [c for c in numeric_cols if c not in drop_cols]\n",
    "\n",
    "# Fill remaining NaNs with median\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "X = data[numeric_cols].astype(np.float32).values\n",
    "y = data['label_enc'].values.astype(np.int32)\n",
    "\n",
    "print(\"Class distribution:\", dict(pd.Series(y).value_counts()))\n",
    "\n",
    "# Optional subsample for demo if huge (you can remove this in full run)\n",
    "MAX_SAMPLES = None  # set to int like 60000 for demo\n",
    "if MAX_SAMPLES is not None and X.shape[0] > MAX_SAMPLES:\n",
    "    _, X, _, y = train_test_split(X, y, train_size=MAX_SAMPLES, stratify=y, random_state=RANDOM_SEED)\n",
    "    print(\"Subsampled to:\", X.shape)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, Path(MODEL_DIR) / \"scaler.joblib\")\n",
    "with open(Path(MODEL_DIR) / \"feature_cols.json\", \"w\") as f:\n",
    "    json.dump(numeric_cols, f)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90aea5fb-0b5a-4ba7-8aad-5c2fe66fd242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ADFNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_features (InputLayer)    [(None, 46)]         0           []                               \n",
      "                                                                                                  \n",
      " att_dense1 (Dense)             (None, 128)          6016        ['input_features[0][0]']         \n",
      "                                                                                                  \n",
      " att_dense2 (Dense)             (None, 46)           5934        ['att_dense1[0][0]']             \n",
      "                                                                                                  \n",
      " att_softmax (Activation)       (None, 46)           0           ['att_dense2[0][0]']             \n",
      "                                                                                                  \n",
      " x_weighted (Multiply)          (None, 46)           0           ['input_features[0][0]',         \n",
      "                                                                  'att_softmax[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           3008        ['x_weighted[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " exit1_logits (Dense)           (None, 35)           2275        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " exit2_logits (Dense)           (None, 35)           1155        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " final_logits (Dense)           (None, 35)           595         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " exit1_prob (Activation)        (None, 35)           0           ['exit1_logits[0][0]']           \n",
      "                                                                                                  \n",
      " exit2_prob (Activation)        (None, 35)           0           ['exit2_logits[0][0]']           \n",
      "                                                                                                  \n",
      " final_prob (Activation)        (None, 35)           0           ['final_logits[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,591\n",
      "Trainable params: 21,591\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Build ADFNet (attention + early exits)\n",
    "# -------------------------\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def build_adfnet(input_dim, num_classes, task_mode):\n",
    "    inputs = layers.Input(shape=(input_dim,), name=\"input_features\")\n",
    "    # Attention MLP\n",
    "    a = layers.Dense(ATTENTION_D, activation=\"relu\", name=\"att_dense1\")(inputs)\n",
    "    a = layers.Dense(input_dim, activation=None, name=\"att_dense2\")(a)\n",
    "    att_weights = layers.Activation(\"softmax\", name=\"att_softmax\")(a)  # shape (batch, input_dim)\n",
    "    x_weighted = layers.Multiply(name=\"x_weighted\")([inputs, att_weights])\n",
    "    # Blocks with early exits\n",
    "    b1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(x_weighted)\n",
    "    exit1_logits = layers.Dense(num_classes if task_mode==\"multiclass\" else 1, name=\"exit1_logits\")(b1)\n",
    "    exit1_prob = layers.Activation(\"softmax\", name=\"exit1_prob\")(\n",
    "        exit1_logits) if task_mode == \"multiclass\" else layers.Activation(\"sigmoid\", name=\"exit1_prob\")(exit1_logits)\n",
    "    b2 = layers.Dense(32, activation=\"relu\", name=\"dense_2\")(b1)\n",
    "    exit2_logits = layers.Dense(num_classes if task_mode==\"multiclass\" else 1, name=\"exit2_logits\")(b2)\n",
    "    exit2_prob = layers.Activation(\"softmax\", name=\"exit2_prob\")(\n",
    "        exit2_logits) if task_mode == \"multiclass\" else layers.Activation(\"sigmoid\", name=\"exit2_prob\")(exit2_logits)\n",
    "    b3 = layers.Dense(16, activation=\"relu\", name=\"dense_3\")(b2)\n",
    "    final_logits = layers.Dense(num_classes if task_mode==\"multiclass\" else 1, name=\"final_logits\")(b3)\n",
    "    final_prob = layers.Activation(\"softmax\", name=\"final_prob\")(\n",
    "        final_logits) if task_mode == \"multiclass\" else layers.Activation(\"sigmoid\", name=\"final_prob\")(final_logits)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=[exit1_prob, exit2_prob, final_prob, att_weights], name=\"ADFNet\")\n",
    "    return model\n",
    "\n",
    "task_mode = TASK_MODE_USED  # \"binary\" or \"multiclass\"\n",
    "num_classes = len(np.unique(y)) if task_mode == \"multiclass\" else 1\n",
    "adf_model = build_adfnet(X_train.shape[1], num_classes, task_mode)\n",
    "adf_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3a4a3f-184a-4e2e-a336-30bf4618fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - loss: 0.317761 - time: 398.1s\n",
      "Epoch 2/30 - loss: 0.102919 - time: 389.9s\n",
      "Epoch 3/30 - loss: 0.087276 - time: 392.3s\n",
      "Epoch 4/30 - loss: 0.082001 - time: 390.9s\n",
      "Epoch 5/30 - loss: 0.080510 - time: 392.9s\n",
      "Epoch 6/30 - loss: 0.076183 - time: 391.6s\n",
      "Epoch 7/30 - loss: 0.072901 - time: 394.7s\n",
      "Epoch 8/30 - loss: 0.071325 - time: 395.4s\n",
      "Epoch 9/30 - loss: 0.067938 - time: 393.8s\n",
      "Epoch 10/30 - loss: 0.069532 - time: 389.4s\n",
      "Epoch 11/30 - loss: 0.059831 - time: 390.2s\n",
      "Epoch 12/30 - loss: 0.056030 - time: 393.9s\n",
      "Epoch 13/30 - loss: 0.055295 - time: 394.7s\n",
      "Epoch 14/30 - loss: 0.049370 - time: 397.8s\n",
      "Epoch 15/30 - loss: 0.050933 - time: 396.4s\n",
      "Epoch 16/30 - loss: 0.046765 - time: 393.4s\n",
      "Epoch 17/30 - loss: 0.047430 - time: 395.2s\n",
      "Epoch 18/30 - loss: 0.046174 - time: 393.5s\n",
      "Epoch 19/30 - loss: 0.045623 - time: 395.5s\n",
      "Epoch 20/30 - loss: 0.044985 - time: 393.8s\n",
      "Epoch 21/30 - loss: 0.045813 - time: 394.7s\n",
      "Epoch 22/30 - loss: 0.044294 - time: 394.6s\n",
      "Epoch 23/30 - loss: 0.043274 - time: 394.6s\n",
      "Epoch 24/30 - loss: 0.043047 - time: 397.1s\n",
      "Epoch 25/30 - loss: 0.041585 - time: 393.9s\n",
      "Epoch 26/30 - loss: 0.042315 - time: 394.3s\n",
      "Epoch 27/30 - loss: 0.044419 - time: 396.2s\n",
      "Epoch 28/30 - loss: 0.042984 - time: 395.5s\n",
      "Epoch 29/30 - loss: 0.040613 - time: 394.0s\n",
      "Epoch 30/30 - loss: 0.041213 - time: 395.2s\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: model\\adfnet_saved\\assets\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Training loop (end-to-end)\n",
    "# -------------------------\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "if task_mode == \"multiclass\":\n",
    "    loss_fn = losses.SparseCategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fn = losses.BinaryCrossentropy()\n",
    "\n",
    "alpha1, alpha2, alpha3 = 0.3, 0.3, 0.4\n",
    "lambda_att = 1e-3\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(BATCH_SIZE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "train_loss_hist = []\n",
    "for epoch in range(EPOCHS):\n",
    "    t0 = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    total = 0\n",
    "    for step, (bx, by) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            e1, e2, f, att = adf_model(bx, training=True)\n",
    "            # compute losses (handle shapes)\n",
    "            if task_mode == \"multiclass\":\n",
    "                l1 = loss_fn(by, e1)\n",
    "                l2 = loss_fn(by, e2)\n",
    "                l3 = loss_fn(by, f)\n",
    "            else:\n",
    "                # e1,e2,f shapes are (batch,1)\n",
    "                l1 = loss_fn(tf.cast(tf.reshape(by, (-1,1)), tf.float32), e1)\n",
    "                l2 = loss_fn(tf.cast(tf.reshape(by, (-1,1)), tf.float32), e2)\n",
    "                l3 = loss_fn(tf.cast(tf.reshape(by, (-1,1)), tf.float32), f)\n",
    "            att_mean = tf.reduce_mean(att)\n",
    "            loss = alpha1*l1 + alpha2*l2 + alpha3*l3 + lambda_att * att_mean\n",
    "        grads = tape.gradient(loss, adf_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, adf_model.trainable_variables))\n",
    "        batch_n = bx.shape[0]\n",
    "        epoch_loss += float(loss.numpy()) * batch_n\n",
    "        total += batch_n\n",
    "    epoch_loss /= total\n",
    "    train_loss_hist.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - loss: {epoch_loss:.6f} - time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Save model (SavedModel folder)\n",
    "adf_model.save(Path(MODEL_DIR) / 'adfnet_saved', include_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263f9109-5dc3-4c8f-8d35-72e4a1da5313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9870382864792503\n",
      "Precision: 0.9854498145331525 Recall: 0.9870382864792503 F1: 0.9856689466354372\n",
      "Confusion matrix:\n",
      " [[    0    17     0 ...     0     0     0]\n",
      " [    0 21412     0 ...     0     0     0]\n",
      " [    0    56     0 ...     0     0     0]\n",
      " ...\n",
      " [    0     0     0 ...   732     0     0]\n",
      " [    0    13     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0    34]]\n",
      "Path distribution: {1: 898583, 3: 34945, 2: 222}\n",
      "Avg latency per sample (s): 0.0001266661028793276\n",
      "Avg attention sparsity (fraction): 0.0868494410463377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Inference with early exit\n",
    "# -------------------------\n",
    "def inference_with_early_exit(model, X_input, exit_thresholds=EARLY_EXIT_THRESHOLDS, batch_size=1024):\n",
    "    n = X_input.shape[0]\n",
    "    y_preds = np.zeros(n, dtype=int)\n",
    "    paths = np.zeros(n, dtype=int)\n",
    "    latencies = []\n",
    "    att_sparsity = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        bx = X_input[i:i+batch_size]\n",
    "        t0 = time.perf_counter()\n",
    "        e1, e2, f, att = model.predict(bx, verbose=0)\n",
    "        t1 = time.perf_counter()\n",
    "        latencies.append((t1-t0)/bx.shape[0])\n",
    "        # e1,e2,f shapes:\n",
    "        # - multiclass: (batch, num_classes)\n",
    "        # - binary: (batch, 1)\n",
    "        for j in range(bx.shape[0]):\n",
    "            if task_mode == \"multiclass\":\n",
    "                if np.max(e1[j]) > exit_thresholds[0]:\n",
    "                    y_preds[i+j] = int(np.argmax(e1[j]))\n",
    "                    paths[i+j] = 1\n",
    "                elif np.max(e2[j]) > exit_thresholds[1]:\n",
    "                    y_preds[i+j] = int(np.argmax(e2[j]))\n",
    "                    paths[i+j] = 2\n",
    "                else:\n",
    "                    y_preds[i+j] = int(np.argmax(f[j]))\n",
    "                    paths[i+j] = 3\n",
    "            else:\n",
    "                # binary: convert prob -> class using 0.5\n",
    "                if e1[j].shape[0] == 1:\n",
    "                    conf1 = float(e1[j][0])\n",
    "                    conf2 = float(e2[j][0])\n",
    "                    conff = float(f[j][0])\n",
    "                else:\n",
    "                    conf1 = float(np.max(e1[j]))\n",
    "                    conf2 = float(np.max(e2[j]))\n",
    "                    conff = float(np.max(f[j]))\n",
    "                if conf1 > exit_thresholds[0]:\n",
    "                    y_preds[i+j] = 1 if conf1 >= 0.5 else 0\n",
    "                    paths[i+j] = 1\n",
    "                elif conf2 > exit_thresholds[1]:\n",
    "                    y_preds[i+j] = 1 if conf2 >= 0.5 else 0\n",
    "                    paths[i+j] = 2\n",
    "                else:\n",
    "                    y_preds[i+j] = 1 if conff >= 0.5 else 0\n",
    "                    paths[i+j] = 3\n",
    "        att_sparsity.append(np.mean(np.count_nonzero(att > (1.0/att.shape[1]), axis=1)/att.shape[1]))\n",
    "    return y_preds, paths, np.mean(latencies), np.mean(att_sparsity)\n",
    "\n",
    "# Run inference\n",
    "y_pred, paths_taken, avg_lat_s, att_s = inference_with_early_exit(adf_model, X_test, exit_thresholds=EARLY_EXIT_THRESHOLDS)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0, average='weighted')\n",
    "rec = recall_score(y_test, y_pred, zero_division=0, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec, \"Recall:\", rec, \"F1:\", f1)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"Path distribution:\", pd.Series(paths_taken).value_counts().to_dict())\n",
    "print(\"Avg latency per sample (s):\", avg_lat_s)\n",
    "print(\"Avg attention sparsity (fraction):\", att_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04223ebb-095f-4453-bcfc-448761932069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mehran\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\mehran\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: ./results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mehran\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Save metrics (safe types)\n",
    "# -------------------------\n",
    "metrics = {\n",
    "    'accuracy': to_py(acc),\n",
    "    'precision': to_py(prec),\n",
    "    'recall': to_py(rec),\n",
    "    'f1': to_py(f1),\n",
    "    'path_dist': {str(int(k)): int(v) for k,v in pd.Series(paths_taken).value_counts().to_dict().items()},\n",
    "    'avg_latency_s': to_py(avg_lat_s),\n",
    "    'attention_sparsity': to_py(att_s)\n",
    "}\n",
    "with open(Path(RESULTS_DIR) / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Save predictions\n",
    "pd.DataFrame({'y_true': y_test.tolist(), 'y_pred': y_pred.tolist(), 'path': paths_taken.tolist()}).to_csv(Path(RESULTS_DIR) / 'predictions.csv', index=False)\n",
    "\n",
    "# Resource metrics\n",
    "total_params = int(np.sum([np.prod(v.shape) for v in adf_model.trainable_variables]))\n",
    "def folder_size(path):\n",
    "    total = 0\n",
    "    for p in Path(path).rglob('*'):\n",
    "        if p.is_file():\n",
    "            total += p.stat().st_size\n",
    "    return total\n",
    "model_size_bytes = folder_size(Path(MODEL_DIR) / 'adfnet_saved') if (Path(MODEL_DIR) / 'adfnet_saved').exists() else 0\n",
    "proc = psutil.Process(os.getpid())\n",
    "mem_mb = proc.memory_info().rss / (1024*1024)\n",
    "\n",
    "class_mapping_serializable = {str(k): int(v) for k, v in class_mapping.items()}\n",
    "\n",
    "with open(Path(RESULTS_DIR) / 'resource_report.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'total_params': int(total_params),\n",
    "        'model_folder_bytes': int(model_size_bytes),\n",
    "        'process_rss_mb': float(mem_mb),\n",
    "        'task_mode': task_mode,\n",
    "        'class_mapping': class_mapping_serializable\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save train loss curve if available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(1, len(train_loss_hist)+1), train_loss_hist, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(Path(RESULTS_DIR) / 'train_loss.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Save classification report\n",
    "try:\n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    with open(Path(RESULTS_DIR) / 'class_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Saved results to:\", RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126a850-2bd4-4aa1-a1e4-eee97167ad6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
